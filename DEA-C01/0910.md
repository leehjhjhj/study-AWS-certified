## 0910 (page ~80)

### 1. AWS Data Exchange - ì„œë“œíŒŒí‹° ë°ì´í„° í†µí•© ìµœì†Œí™”

**ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤**: ë¯¸ë””ì–´ íšŒì‚¬ê°€ ì¶”ì²œ ì‹œìŠ¤í…œ ê°œì„ ì„ ìœ„í•´ ì„œë“œíŒŒí‹° ë°ì´í„°ì…‹ì„ ê¸°ì¡´ ë¶„ì„ í”Œë«í¼ì— í†µí•©, ìµœì†Œí•œì˜ ìš´ì˜ ì˜¤ë²„í—¤ë“œë¡œ êµ¬í˜„

**ì •ë‹µ: A. AWS Data Exchangeì—ì„œ API í˜¸ì¶œë¡œ ì„œë“œíŒŒí‹° ë°ì´í„°ì…‹ ì ‘ê·¼ ë° í†µí•©**

**AWS Data Exchangeë€?**

**A. AWS Data Exchange í•µì‹¬ ê°œë…:**
- **ë°ì´í„° ë§ˆì¼“í”Œë ˆì´ìŠ¤**: ì„œë“œíŒŒí‹° ë°ì´í„° ê³µê¸‰ìì™€ ì†Œë¹„ìë¥¼ ì—°ê²°
- **ê´€ë¦¬í˜• ì„œë¹„ìŠ¤**: AWSê°€ ë°ì´í„° ì „ì†¡, ê³¼ê¸ˆ, ë¼ì´ì„¼ìŠ¤ ê´€ë¦¬
- **API ê¸°ë°˜ ì ‘ê·¼**: í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ì…‹ êµ¬ë… ë° ì ‘ê·¼
- **ìë™ ì—…ë°ì´íŠ¸**: ë°ì´í„° ê³µê¸‰ìê°€ ì—…ë°ì´íŠ¸í•˜ë©´ ìë™ìœ¼ë¡œ ìµœì‹  ë°ì´í„° ì œê³µ

**B. ìš´ì˜ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™” ì¥ì :**
```python
# ê¸°ì¡´ ë°©ì‹ (ë³µì¡í•œ í†µí•©)
# 1. ì„œë“œíŒŒí‹° API ì—°ë™ ê°œë°œ
# 2. ì¸ì¦/ì¸ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•
# 3. ë°ì´í„° í˜•ì‹ ë³€í™˜ ë¡œì§
# 4. ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
# 5. ë°ì´í„° ì—…ë°ì´íŠ¸ ê°ì§€ ì‹œìŠ¤í…œ

# Data Exchange ë°©ì‹ (ê°„ë‹¨í•œ í†µí•©)
import boto3

data_exchange = boto3.client('dataexchange')

# êµ¬ë…í•œ ë°ì´í„°ì…‹ ìë™ ë‹¤ìš´ë¡œë“œ
response = data_exchange.get_asset(
    AssetId='asset-id',
    DataSetId='dataset-id', 
    RevisionId='revision-id'
)
```

**C. í†µí•© ì•„í‚¤í…ì²˜:**
```
ì„œë“œíŒŒí‹° ë°ì´í„° â†’ Data Exchange â†’ S3 â†’ Glue ETL â†’ ê¸°ì¡´ ë¶„ì„ í”Œë«í¼
       â†‘              â†‘           â†‘        â†‘           â†‘
    ì™¸ë¶€ ë°ì´í„°      ê´€ë¦¬í˜• ì„œë¹„ìŠ¤   ìë™ ì €ì¥   ë³€í™˜ ì²˜ë¦¬    ì¶”ì²œ ì‹œìŠ¤í…œ
```

ì‹œí—˜ í¬ì¸íŠ¸:
- **ì„œë“œíŒŒí‹° ë°ì´í„° í†µí•©** = AWS Data Exchange í™œìš©
- **ìµœì†Œ ìš´ì˜ ì˜¤ë²„í—¤ë“œ** = ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ì˜ ìë™í™”ëœ ë°ì´í„° ì „ì†¡
- **API ê¸°ë°˜ ì ‘ê·¼** = í”„ë¡œê·¸ë˜ë° ë°©ì‹ì˜ ê°„í¸í•œ ë°ì´í„° ì†Œë¹„

### 2. Amazon Athena íŒŒí‹°ì…˜ ì„±ëŠ¥ ìµœì í™” - ì¸ë±ìŠ¤ì™€ í”„ë¡œì ì…˜

**ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤**: Athena ì¿¼ë¦¬ì—ì„œ S3ì˜ ë§ì€ íŒŒí‹°ì…˜ìœ¼ë¡œ ì¸í•œ ì¿¼ë¦¬ í”Œëœ ì„±ëŠ¥ ë³‘ëª©, ì¿¼ë¦¬ ê³„íš ì‹œê°„ ë‹¨ì¶• í•„ìš”

**ì •ë‹µ: A. AWS Glue íŒŒí‹°ì…˜ ì¸ë±ìŠ¤ ìƒì„± + íŒŒí‹°ì…˜ í•„í„°ë§ í™œì„±í™” + C. S3 ë²„í‚· prefix ê¸°ë°˜ Athena íŒŒí‹°ì…˜ í”„ë¡œì ì…˜**

**A. AWS Glue íŒŒí‹°ì…˜ ì¸ë±ìŠ¤ì˜ íš¨ê³¼:**

**íŒŒí‹°ì…˜ ì¸ë±ìŠ¤ ìƒì„±:**
```python
import boto3

glue = boto3.client('glue')

# íŒŒí‹°ì…˜ ì¸ë±ìŠ¤ ìƒì„±
response = glue.create_partition_index(
    DatabaseName='analytics_db',
    TableName='user_events',
    PartitionIndex={
        'IndexName': 'date-region-index',
        'Keys': [
            {'Name': 'year', 'Type': 'string'},
            {'Name': 'month', 'Type': 'string'}, 
            {'Name': 'region', 'Type': 'string'}
        ]
    }
)
```

**ì„±ëŠ¥ ê°œì„  íš¨ê³¼:**
```
íŒŒí‹°ì…˜ ì¸ë±ìŠ¤ ì—†ìŒ:
10,000ê°œ íŒŒí‹°ì…˜ â†’ ì „ì²´ íŒŒí‹°ì…˜ ë©”íƒ€ë°ì´í„° ìŠ¤ìº” â†’ 30ì´ˆ ì¿¼ë¦¬ í”Œë˜ë‹

íŒŒí‹°ì…˜ ì¸ë±ìŠ¤ ì ìš©:
10,000ê°œ íŒŒí‹°ì…˜ â†’ ì¸ë±ìŠ¤ ê¸°ë°˜ í•„í„°ë§ â†’ 50ê°œ ê´€ë ¨ íŒŒí‹°ì…˜ë§Œ í™•ì¸ â†’ 2ì´ˆ ì¿¼ë¦¬ í”Œë˜ë‹
```

**B. Athena íŒŒí‹°ì…˜ í”„ë¡œì ì…˜ì˜ ì¥ì :**

**íŒŒí‹°ì…˜ í”„ë¡œì ì…˜ ì„¤ì •:**
```sql
-- í…Œì´ë¸” í”„ë¡œí¼í‹°ì— íŒŒí‹°ì…˜ í”„ë¡œì ì…˜ ì„¤ì •
ALTER TABLE user_events SET TBLPROPERTIES (
  'projection.enabled' = 'true',
  'projection.year.type' = 'integer',
  'projection.year.range' = '2020,2030',
  'projection.month.type' = 'integer', 
  'projection.month.range' = '1,12',
  'projection.month.digits' = '2',
  'projection.day.type' = 'integer',
  'projection.day.range' = '1,31',
  'projection.day.digits' = '2',
  'storage.location.template' = 's3://bucket/data/year=${year}/month=${month}/day=${day}/'
);
```

**í”„ë¡œì ì…˜ ì‘ë™ ì›ë¦¬:**
```
ê¸°ì¡´ ë°©ì‹ (ë©”íƒ€ìŠ¤í† ì–´ ì˜ì¡´):
ì¿¼ë¦¬ ì‹¤í–‰ â†’ Glue Catalog íŒŒí‹°ì…˜ ëª©ë¡ ì¡°íšŒ â†’ íŒŒí‹°ì…˜ í•„í„°ë§ â†’ S3 ì ‘ê·¼

íŒŒí‹°ì…˜ í”„ë¡œì ì…˜ (ë©”íƒ€ìŠ¤í† ì–´ ìš°íšŒ):
ì¿¼ë¦¬ ì‹¤í–‰ â†’ íŒ¨í„´ ê¸°ë°˜ íŒŒí‹°ì…˜ ê³„ì‚° â†’ ì§ì ‘ S3 ì ‘ê·¼
```

**C. ë‘ ì†”ë£¨ì…˜ì˜ ì‹œë„ˆì§€ íš¨ê³¼:**

**ë³µí•© ìµœì í™” ì „ëµ:**
1. **íŒŒí‹°ì…˜ ì¸ë±ìŠ¤**: ê¸°ì¡´ íŒŒí‹°ì…˜ë“¤ì— ëŒ€í•œ ë¹ ë¥¸ ë©”íƒ€ë°ì´í„° ì ‘ê·¼
2. **íŒŒí‹°ì…˜ í”„ë¡œì ì…˜**: ìƒˆë¡œìš´ íŒŒí‹°ì…˜ë“¤ì— ëŒ€í•œ ë©”íƒ€ìŠ¤í† ì–´ ìš°íšŒ ì ‘ê·¼

**ì„±ëŠ¥ ë¹„êµ:**
```
ìµœì í™” ì „: 10,000 íŒŒí‹°ì…˜ â†’ 30ì´ˆ ì¿¼ë¦¬ í”Œë˜ë‹
íŒŒí‹°ì…˜ ì¸ë±ìŠ¤ë§Œ: 10,000 íŒŒí‹°ì…˜ â†’ 2ì´ˆ ì¿¼ë¦¬ í”Œë˜ë‹  
í”„ë¡œì ì…˜ë§Œ: ë©”íƒ€ìŠ¤í† ì–´ ìš°íšŒ â†’ 0.5ì´ˆ ì¿¼ë¦¬ í”Œë˜ë‹
ë‘˜ ë‹¤ ì ìš©: ìµœì ì˜ ë°©ë²• ìë™ ì„ íƒ â†’ í‰ê·  0.3ì´ˆ ì¿¼ë¦¬ í”Œë˜ë‹
```

ì‹œí—˜ í¬ì¸íŠ¸:
- **íŒŒí‹°ì…˜ ì„±ëŠ¥ ë³‘ëª©** = íŒŒí‹°ì…˜ ì¸ë±ìŠ¤ + íŒŒí‹°ì…˜ í”„ë¡œì ì…˜
- **ë©”íƒ€ë°ì´í„° ìµœì í™”** = Glue Catalog ì¸ë±ìŠ¤ í™œìš©
- **ë©”íƒ€ìŠ¤í† ì–´ ìš°íšŒ** = íŒŒí‹°ì…˜ í”„ë¡œì ì…˜ìœ¼ë¡œ ì§ì ‘ S3 ì ‘ê·¼

### 3. Amazon Redshift Data Sharing - í´ëŸ¬ìŠ¤í„° ê°„ ë°ì´í„° ê³µìœ 

**ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤**: ETLìš© Redshift í´ëŸ¬ìŠ¤í„°ì˜ ë°ì´í„°ë¥¼ BIìš© Redshift í´ëŸ¬ìŠ¤í„°ì™€ ê³µìœ , ì¤‘ìš”í•œ ë¶„ì„ ì‘ì—… ì¤‘ë‹¨ ì—†ì´ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ìµœì†Œí™”

**ì •ë‹µ: A. Redshift ë°ì´í„° ê³µìœ ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ì—…íŒ€ BI í´ëŸ¬ìŠ¤í„°ë¥¼ ETL í´ëŸ¬ìŠ¤í„°ì˜ ì†Œë¹„ìë¡œ ì„¤ì •**

**A. Redshift Data Sharing í•µì‹¬ ê°œë…:**

**ë°ì´í„° ê³µìœ  ì•„í‚¤í…ì²˜:**
```
Producer Cluster (ETL)     Consumer Cluster (BI)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì‹¤ì œ ë°ì´í„° ì €ì¥     â”‚ â”€â”€â–º â”‚ ê°€ìƒ ë·°ë¡œ ë°ì´í„° ì ‘ê·¼ â”‚
â”‚ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©   â”‚    â”‚ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë§Œ ì‚¬ìš©  â”‚
â”‚ ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ë°œìƒ   â”‚    â”‚ ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ì—†ìŒ    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**B. ë°ì´í„° ê³µìœ  ì„¤ì •:**

**Producer ì¸¡ ì„¤ì • (ETL í´ëŸ¬ìŠ¤í„°):**
```sql
-- ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
CREATE NAMESPACE etl_data_share;

-- ë°ì´í„° ê³µìœ  ìƒì„±
CREATE DATASHARE sales_data_share;

-- í…Œì´ë¸”ì„ ë°ì´í„° ê³µìœ ì— ì¶”ê°€
ALTER DATASHARE sales_data_share 
ADD SCHEMA public;

ALTER DATASHARE sales_data_share 
ADD TABLE public.orders, public.customers, public.products;

-- ì†Œë¹„ì í´ëŸ¬ìŠ¤í„°ì— ê¶Œí•œ ë¶€ì—¬
GRANT USAGE ON DATASHARE sales_data_share 
TO NAMESPACE 'consumer-cluster-namespace';
```

**Consumer ì¸¡ ì„¤ì • (BI í´ëŸ¬ìŠ¤í„°):**
```sql
-- ë°ì´í„° ê³µìœ ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
CREATE DATABASE shared_sales 
FROM DATASHARE sales_data_share 
OF NAMESPACE 'producer-cluster-namespace';

-- ê³µìœ ëœ ë°ì´í„°ë¥¼ ë¡œì»¬ ë°ì´í„°ì™€ ì¡°ì¸
SELECT s.order_date, s.amount, l.campaign_id
FROM shared_sales.public.orders s
JOIN local_bi.public.campaigns l 
ON s.customer_id = l.customer_id;
```

**C. ì™œ ì´ ì†”ë£¨ì…˜ì´ ìµœì ì¸ê°€:**

**ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ìµœì†Œí™”:**
- âœ… **Producer ë¶€í•˜ ì—†ìŒ**: ETL í´ëŸ¬ìŠ¤í„°ëŠ” ë°ì´í„°ë§Œ ê³µìœ , ì¶”ê°€ ì¿¼ë¦¬ ì²˜ë¦¬ ì•ˆí•¨
- âœ… **ì‹¤ì‹œê°„ ì ‘ê·¼**: ë³„ë„ ë°ì´í„° ë³µì‚¬ë‚˜ ETL ê³¼ì • ë¶ˆí•„ìš”
- âœ… **ìŠ¤í† ë¦¬ì§€ ê³µìœ **: ë™ì¼ ë°ì´í„°ë¥¼ ì¤‘ë³µ ì €ì¥í•˜ì§€ ì•ŠìŒ

**ì¤‘ë‹¨ ì—†ëŠ” ìš´ì˜:**
- **ë…ë¦½ì  ì¿¼ë¦¬ ì‹¤í–‰**: BI í´ëŸ¬ìŠ¤í„°ì˜ ì¿¼ë¦¬ê°€ ETL í´ëŸ¬ìŠ¤í„°ì— ì˜í–¥ ì—†ìŒ
- **ìë™ ë™ê¸°í™”**: Producer ë°ì´í„° ë³€ê²½ ì‹œ Consumerì— ì¦‰ì‹œ ë°˜ì˜
- **ê¶Œí•œ ì œì–´**: íŠ¹ì • í…Œì´ë¸”/ìŠ¤í‚¤ë§ˆë§Œ ì„ íƒì  ê³µìœ  ê°€ëŠ¥

**D. ë‹¤ë¥¸ ì˜µì…˜ë“¤ê³¼ ë¹„êµ:**

**ë°ì´í„° ë³µì‚¬ ë°©ì‹ (ì˜¤ë‹µ):**
- âŒ **ìŠ¤í† ë¦¬ì§€ ì¤‘ë³µ**: ë™ì¼ ë°ì´í„°ë¥¼ ë‘ ë²ˆ ì €ì¥
- âŒ **ë™ê¸°í™” ë³µì¡ì„±**: ETLê³¼ BI ê°„ ë°ì´í„° ì¼ê´€ì„± ê´€ë¦¬ í•„ìš”
- âŒ **ë¹„ìš© ì¦ê°€**: ìŠ¤í† ë¦¬ì§€ ë¹„ìš© 2ë°°

**Federated Query (ì˜¤ë‹µ):**  
- âŒ **ì„±ëŠ¥ ì €í•˜**: ì‹¤ì‹œê°„ ë°ì´í„° ì „ì†¡ìœ¼ë¡œ ì§€ì—° ë°œìƒ
- âŒ **ë„¤íŠ¸ì›Œí¬ ë¹„ìš©**: í´ëŸ¬ìŠ¤í„° ê°„ ëŒ€ëŸ‰ ë°ì´í„° ì´ë™
- âŒ **ë³µì¡í•œ ì¡°ì¸**: í¬ë¡œìŠ¤ í´ëŸ¬ìŠ¤í„° ì¡°ì¸ ì„±ëŠ¥ ë¬¸ì œ

**E. ë¹„ìš© ë° ì„±ëŠ¥ ë¶„ì„:**

**ë¹„ìš© êµ¬ì¡°:**
```
ê¸°ì¡´ ë°©ì‹ (ë°ì´í„° ë³µì‚¬):
ETL í´ëŸ¬ìŠ¤í„°: $1000/ì›” (4ë…¸ë“œ)
BI í´ëŸ¬ìŠ¤í„°: $800/ì›” (3ë…¸ë“œ) + $500/ì›” (ì¤‘ë³µ ìŠ¤í† ë¦¬ì§€)
ì´ ë¹„ìš©: $2300/ì›”

Data Sharing ë°©ì‹:
ETL í´ëŸ¬ìŠ¤í„°: $1000/ì›” (4ë…¸ë“œ, ê³µìœ  ì„¤ì •ë§Œ)
BI í´ëŸ¬ìŠ¤í„°: $800/ì›” (3ë…¸ë“œ, ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ì—†ìŒ)  
ì´ ë¹„ìš©: $1800/ì›” (22% ì ˆì•½)
```

**ì„±ëŠ¥ ìµœì í™”:**
- **ì§€ì—° ì‹œê°„**: ì‹¤ì‹œê°„ ë°ì´í„° ì ‘ê·¼ (ë³µì‚¬ ì§€ì—° ì—†ìŒ)
- **ì¼ê´€ì„±**: ë‹¨ì¼ ì›ë³¸ìœ¼ë¡œ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥
- **í™•ì¥ì„±**: Producer/Consumer ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§

ì‹œí—˜ í¬ì¸íŠ¸:
- **í´ëŸ¬ìŠ¤í„° ê°„ ë°ì´í„° ê³µìœ ** = Redshift Data Sharing
- **ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ìµœì†Œí™”** = Producer í´ëŸ¬ìŠ¤í„° ë¶€í•˜ ì—†ìŒ
- **ì‹¤ì‹œê°„ ë°ì´í„° ì ‘ê·¼** = ë³„ë„ ë³µì‚¬ ì—†ì´ ì¦‰ì‹œ ì ‘ê·¼

### 4. Amazon EMR Graviton ì¸ìŠ¤í„´ìŠ¤ - ARM ê¸°ë°˜ ì„±ëŠ¥ ìµœì í™”

**ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤**: EMR í´ëŸ¬ìŠ¤í„°ì˜ ë¹„ìš© ìµœì í™”ì™€ ì„±ëŠ¥ í–¥ìƒ

**ì •ë‹µ: D. Core ë…¸ë“œì™€ Task ë…¸ë“œì— Graviton ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©**

**A. Graviton ì¸ìŠ¤í„´ìŠ¤ë€?**

**ARM ê¸°ë°˜ í”„ë¡œì„¸ì„œ:**
- **AWS Graviton2/3**: ARM Neoverse ê¸°ë°˜ ì»¤ìŠ¤í…€ ì‹¤ë¦¬ì½˜
- **ì—ë„ˆì§€ íš¨ìœ¨ì„±**: x86 ëŒ€ë¹„ 20% ë” ë†’ì€ ì„±ëŠ¥/ì „ë ¥ ë¹„ìœ¨
- **ë¹„ìš© íš¨ìœ¨ì„±**: ë™ë“± ì„±ëŠ¥ ëŒ€ë¹„ ìµœëŒ€ 40% ë¹„ìš© ì ˆê°
- **EMR ìµœì í™”**: Spark, Hadoop, Hive ë“± ë¹…ë°ì´í„° ì›Œí¬ë¡œë“œ ìµœì í™”

**B. EMRì—ì„œ Graviton ì¸ìŠ¤í„´ìŠ¤ í™œìš©:**

**ì§€ì›ë˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…:**
```json
{
  "MasterInstanceType": "m6g.xlarge",    // Graviton2 ë²”ìš©
  "CoreInstanceType": "m6g.2xlarge",     // Graviton2 ë²”ìš©  
  "TaskInstanceType": "c6g.2xlarge"      // Graviton2 ì»´í“¨íŒ… ìµœì í™”
}
```

**ì„±ëŠ¥/ë¹„ìš© ë¹„êµ:**
```
x86 ì¸ìŠ¤í„´ìŠ¤ (ê¸°ì¡´):
m5.2xlarge: 8 vCPU, 32GB RAM, $0.384/ì‹œê°„
c5.2xlarge: 8 vCPU, 16GB RAM, $0.34/ì‹œê°„

Graviton ì¸ìŠ¤í„´ìŠ¤ (ìµœì í™”):
m6g.2xlarge: 8 vCPU, 32GB RAM, $0.308/ì‹œê°„ (20% ì ˆì•½)
c6g.2xlarge: 8 vCPU, 16GB RAM, $0.272/ì‹œê°„ (20% ì ˆì•½)
```

**C. EMR ì›Œí¬ë¡œë“œë³„ ì„±ëŠ¥ í–¥ìƒ:**

**Spark ì‘ì—… ìµœì í™”:**
```python
# Spark ì„¤ì • ìµœì í™” (Graviton ì¸ìŠ¤í„´ìŠ¤ìš©)
spark_config = {
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.coalescePartitions.enabled": "true",
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
    "spark.executor.cores": "4",  # Gravitonì˜ ë©€í‹°ì½”ì–´ í™œìš©
    "spark.executor.memory": "6g"
}
```

**ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:**
```
ë™ì¼ Spark ì‘ì—… (10GB ë°ì´í„° ì²˜ë¦¬):
x86 ì¸ìŠ¤í„´ìŠ¤: 45ë¶„
Graviton ì¸ìŠ¤í„´ìŠ¤: 38ë¶„ (15% ì„±ëŠ¥ í–¥ìƒ)

ë¹„ìš© ë¶„ì„ (100ë…¸ë“œ í´ëŸ¬ìŠ¤í„°, ì¼ 8ì‹œê°„):
x86: $307.2/ì¼
Graviton: $246.4/ì¼ (20% ë¹„ìš© ì ˆê°)
```

**D. Graviton ì‚¬ìš© ì‹œ ê³ ë ¤ì‚¬í•­:**

**í˜¸í™˜ì„±:**
- âœ… **EMR 6.1+**: Graviton ì¸ìŠ¤í„´ìŠ¤ ì™„ì „ ì§€ì›
- âœ… **ì£¼ìš” ë¹…ë°ì´í„° ë„êµ¬**: Spark, Hadoop, Hive, HBase ëª¨ë‘ ì§€ì›
- âœ… **Java/Scala ì• í”Œë¦¬ì¼€ì´ì…˜**: JVM ê¸°ë°˜ ì•± ì™„ë²½ í˜¸í™˜
- â“ **ë„¤ì´í‹°ë¸Œ ë°”ì´ë„ˆë¦¬**: ARMìš© ì»´íŒŒì¼ í•„ìš” (ë“œë¬¼ê²Œ ë°œìƒ)

**ìµœì í™”ëœ êµ¬ì„±:**
```bash
# EMR í´ëŸ¬ìŠ¤í„° ìƒì„± (Graviton ì¸ìŠ¤í„´ìŠ¤)
aws emr create-cluster \
  --name "Graviton-Optimized-Cluster" \
  --release-label emr-6.4.0 \
  --instance-groups \
    InstanceGroupType=MASTER,InstanceType=m6g.xlarge,InstanceCount=1 \
    InstanceGroupType=CORE,InstanceType=m6g.2xlarge,InstanceCount=4 \
    InstanceGroupType=TASK,InstanceType=c6g.2xlarge,InstanceCount=8 \
  --applications Name=Spark Name=Hadoop
```

**E. ë¹„ìš© ìµœì í™” íš¨ê³¼:**

**ì›”ê°„ ë¹„ìš© ë¶„ì„ (ì¤‘ê°„ ê·œëª¨ í´ëŸ¬ìŠ¤í„°):**
```
ê¸°ì¡´ x86 êµ¬ì„±:
Master: m5.xlarge Ã— 1 = $140/ì›”
Core: m5.2xlarge Ã— 4 = $1,107/ì›”  
Task: c5.2xlarge Ã— 8 = $1,958/ì›”
ì´ ë¹„ìš©: $3,205/ì›”

Graviton êµ¬ì„±:
Master: m6g.xlarge Ã— 1 = $112/ì›” (20% ì ˆì•½)
Core: m6g.2xlarge Ã— 4 = $886/ì›” (20% ì ˆì•½)
Task: c6g.2xlarge Ã— 8 = $1,566/ì›” (20% ì ˆì•½)  
ì´ ë¹„ìš©: $2,564/ì›” (ì „ì²´ 20% ì ˆì•½)

ì—°ê°„ ì ˆì•½ì•¡: $7,692
```

ì‹œí—˜ í¬ì¸íŠ¸:
- **EMR ë¹„ìš© ìµœì í™”** = Graviton ì¸ìŠ¤í„´ìŠ¤ í™œìš©
- **ARM ê¸°ë°˜ ì„±ëŠ¥ í–¥ìƒ** = ì—ë„ˆì§€ íš¨ìœ¨ì  í”„ë¡œì„¸ì„œ
- **ë¹…ë°ì´í„° ì›Œí¬ë¡œë“œ** = Spark, Hadoop ë“± ì™„ì „ í˜¸í™˜

### 5. EC2 ë°ì´í„° ì˜ì†ì„± - EBS ë³¼ë¥¨ ì—°ê²°

**ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤**: EC2 ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ìƒì„±ë˜ëŠ” ë°ì´í„°ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ í›„ì—ë„ ë³´ì¡´, AMIì—ì„œ ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘ ì‹œ ë°ì´í„° ìœ ì§€

**ì •ë‹µ: EC2 ì¸ìŠ¤í„´ìŠ¤ ìŠ¤í† ì–´ ë³¼ë¥¨ ê¸°ë°˜ AMIë¡œ ìƒˆ EC2 ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘ + ì• í”Œë¦¬ì¼€ì´ì…˜ ë°ì´í„°ìš© Amazon EBS ë³¼ë¥¨ ì—°ê²° + EC2 ì¸ìŠ¤í„´ìŠ¤ì— ê¸°ë³¸ ì„¤ì • ì ìš©**

**A. ì¸ìŠ¤í„´ìŠ¤ ìŠ¤í† ì–´ vs EBSì˜ ì´í•´:**

**ì¸ìŠ¤í„´ìŠ¤ ìŠ¤í† ì–´ ë³¼ë¥¨:**
- **ì„ì‹œ ìŠ¤í† ë¦¬ì§€**: ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ ì‹œ ë°ì´í„° ì†Œì‹¤
- **ê³ ì„±ëŠ¥**: ë¬¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°ëœ SSD, ë†’ì€ I/O ì„±ëŠ¥
- **ë¬´ë£Œ**: ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ì— í¬í•¨ëœ ìŠ¤í† ë¦¬ì§€
- **ìš©ë„**: OS, ì• í”Œë¦¬ì¼€ì´ì…˜, ì„ì‹œ íŒŒì¼

**EBS ë³¼ë¥¨:**
- **ì˜êµ¬ ìŠ¤í† ë¦¬ì§€**: ì¸ìŠ¤í„´ìŠ¤ì™€ ë…ë¦½ì ìœ¼ë¡œ ë°ì´í„° ë³´ì¡´
- **ìŠ¤ëƒ…ìƒ· ì§€ì›**: ë°±ì—… ë° ë³µì› ê°€ëŠ¥
- **í¬ê¸° ì¡°ì ˆ**: ë™ì ìœ¼ë¡œ ë³¼ë¥¨ í¬ê¸° ë³€ê²½ ê°€ëŠ¥
- **ìš©ë„**: ì¤‘ìš”í•œ ë°ì´í„°, ë°ì´í„°ë² ì´ìŠ¤, ë¡œê·¸

**B. ì™œ ì´ êµ¬ì„±ì´ ì •ë‹µì¸ê°€:**

**ë°ì´í„° ë¶„ë¦¬ ì „ëµ:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Instance Store      â”‚    â”‚ EBS Volume           â”‚
â”‚ - OS (ì„ì‹œ)         â”‚    â”‚ - Application Data   â”‚
â”‚ - Application       â”‚    â”‚ - ì˜êµ¬ ë³´ì¡´          â”‚
â”‚ - Temp Files        â”‚    â”‚ - ë°±ì—… ê°€ëŠ¥          â”‚
â”‚ (ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œì‹œ ì‚­ì œ)â”‚    â”‚ (ì¸ìŠ¤í„´ìŠ¤ì™€ ë…ë¦½ì )   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì˜ì†ì„± ë³´ì¥:**
- **ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ**: ì¸ìŠ¤í„´ìŠ¤ ìŠ¤í† ì–´ëŠ” ì‚­ì œ, EBSëŠ” ë³´ì¡´
- **ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘**: ë™ì¼ EBS ë³¼ë¥¨ ì¬ì—°ê²°ë¡œ ë°ì´í„° ë³µì›
- **AMI í™œìš©**: í‘œì¤€í™”ëœ í™˜ê²½ ë¹ ë¥¸ êµ¬ì¶•

**C. êµ¬í˜„ ì˜ˆì‹œ:**

**EBS ë³¼ë¥¨ ì—°ê²°:**
```bash
# EBS ë³¼ë¥¨ ìƒì„±
aws ec2 create-volume \
  --size 100 \
  --volume-type gp3 \
  --availability-zone us-east-1a \
  --tag-specifications 'ResourceType=volume,Tags=[{Key=Name,Value=AppData}]'

# ì¸ìŠ¤í„´ìŠ¤ì— ë³¼ë¥¨ ì—°ê²°
aws ec2 attach-volume \
  --volume-id vol-12345678 \
  --instance-id i-87654321 \
  --device /dev/sdf

# íŒŒì¼ì‹œìŠ¤í…œ ìƒì„± ë° ë§ˆìš´íŠ¸
sudo mkfs -t ext4 /dev/xvdf
sudo mkdir /app-data
sudo mount /dev/xvdf /app-data

# ë¶€íŒ…ì‹œ ìë™ ë§ˆìš´íŠ¸ ì„¤ì •
echo '/dev/xvdf /app-data ext4 defaults,nofail 0 2' >> /etc/fstab
```

**ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •:**
```python
# ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ EBS ë³¼ë¥¨ ì‚¬ìš©
import os

# ë°ì´í„° ì €ì¥ ê²½ë¡œë¥¼ EBS ë³¼ë¥¨ìœ¼ë¡œ ì„¤ì •
DATA_PATH = '/app-data'  # EBS ë³¼ë¥¨ ë§ˆìš´íŠ¸ í¬ì¸íŠ¸
TEMP_PATH = '/tmp'       # ì¸ìŠ¤í„´ìŠ¤ ìŠ¤í† ì–´ í™œìš©

def save_important_data(data):
    # ì¤‘ìš”í•œ ë°ì´í„°ëŠ” EBSì— ì €ì¥
    with open(f'{DATA_PATH}/important_data.json', 'w') as f:
        json.dump(data, f)

def save_temp_data(data):
    # ì„ì‹œ ë°ì´í„°ëŠ” ì¸ìŠ¤í„´ìŠ¤ ìŠ¤í† ì–´ì— ì €ì¥
    with open(f'{TEMP_PATH}/temp_data.json', 'w') as f:
        json.dump(data, f)
```

**D. ë‹¤ë¥¸ ì˜µì…˜ë“¤ì´ ë¶€ì ì ˆí•œ ì´ìœ :**

**ëª¨ë“  ë°ì´í„°ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ìŠ¤í† ì–´ì— ì €ì¥ (ì˜¤ë‹µ):**
- âŒ **ë°ì´í„° ì†Œì‹¤**: ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ ì‹œ ëª¨ë“  ë°ì´í„° ì˜êµ¬ ì‚­ì œ
- âŒ **ë°±ì—… ë¶ˆê°€**: ìŠ¤ëƒ…ìƒ·ì´ë‚˜ ë°±ì—… ê¸°ëŠ¥ ì—†ìŒ

**EBS ë£¨íŠ¸ ë³¼ë¥¨ë§Œ ì‚¬ìš© (ë¶€ë¶„ì ):**
- ğŸ”¶ **ê°€ëŠ¥í•˜ì§€ë§Œ ë¹„íš¨ìœ¨ì **: OSì™€ ë°ì´í„°ê°€ í˜¼ì¬
- ğŸ”¶ **ì„±ëŠ¥ ì €í•˜**: ë‹¨ì¼ ë³¼ë¥¨ì— ëª¨ë“  I/O ì§‘ì¤‘
- ğŸ”¶ **ê´€ë¦¬ ë³µì¡ì„±**: OS ì—…ê·¸ë ˆì´ë“œ ì‹œ ë°ì´í„° ì˜í–¥

**E. ìš´ì˜ ê³ ë ¤ì‚¬í•­:**

**ë°±ì—… ì „ëµ:**
```bash
# ì •ê¸°ì  ìŠ¤ëƒ…ìƒ· ìƒì„±
aws ec2 create-snapshot \
  --volume-id vol-12345678 \
  --description "Daily backup of application data"

# ìë™ ë°±ì—… ìŠ¤ì¼€ì¤„ (Lambda + CloudWatch Events)
# ë§¤ì¼ ìì •ì— ìŠ¤ëƒ…ìƒ· ìƒì„± ë° ì˜¤ë˜ëœ ìŠ¤ëƒ…ìƒ· ì‚­ì œ
```

**ëª¨ë‹ˆí„°ë§:**
```python
# CloudWatch ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§
import boto3

cloudwatch = boto3.client('cloudwatch')

# EBS ë³¼ë¥¨ ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
cloudwatch.put_metric_alarm(
    AlarmName='EBS-Volume-Usage',
    ComparisonOperator='GreaterThanThreshold',
    EvaluationPeriods=2,
    MetricName='VolumeUtilization',
    Threshold=80.0,  # 80% ì‚¬ìš© ì‹œ ì•ŒëŒ
    ActionsEnabled=True
)
```

ì‹œí—˜ í¬ì¸íŠ¸:
- **ë°ì´í„° ì˜ì†ì„±** = EBS ë³¼ë¥¨ìœ¼ë¡œ ì¤‘ìš” ë°ì´í„° ë¶„ë¦¬
- **ì¸ìŠ¤í„´ìŠ¤ ìŠ¤í† ì–´ í™œìš©** = OSì™€ ì„ì‹œ íŒŒì¼ìš©
- **í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤í† ë¦¬ì§€** = ì„±ëŠ¥ê³¼ ì˜ì†ì„±ì˜ ê· í˜•

### 6. AWS Glue FLEX ì‹¤í–‰ í´ë˜ìŠ¤ - ë¹„ìš© ìµœì í™”

**ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤**: AWS Glue ì‘ì—…ì„ ë§¤ì¼ ì‹¤í–‰í•˜ë˜ íŠ¹ì • ì‹œê°„ì— ì‹¤í–‰í•˜ê±°ë‚˜ ì™„ë£Œí•  í•„ìš” ì—†ìŒ, ê°€ì¥ ë¹„ìš© íš¨ìœ¨ì ì¸ ë°©ë²• í•„ìš”

**ì •ë‹µ: Glue ì‘ì—… ì†ì„±ì—ì„œ FLEX ì‹¤í–‰ í´ë˜ìŠ¤ ì„ íƒ**

**A. Glue FLEX ì‹¤í–‰ í´ë˜ìŠ¤ë€?**

**ì‹¤í–‰ í´ë˜ìŠ¤ ë¹„êµ:**
- **Standard í´ë˜ìŠ¤**: ì¦‰ì‹œ ì‹¤í–‰, ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì‹œì‘ ì‹œê°„
- **FLEX í´ë˜ìŠ¤**: ì§€ì—°ëœ ì‹¤í–‰, ë¹„ìš© ìµœì í™” ìš°ì„ 

**FLEX í´ë˜ìŠ¤ íŠ¹ì§•:**
```python
# FLEX ì‹¤í–‰ í´ë˜ìŠ¤ ì„¤ì •
job_config = {
    "Name": "cost-optimized-etl",
    "ExecutionClass": "FLEX",  # ë¹„ìš© ìµœì í™” ëª¨ë“œ
    "MaxCapacity": 10.0,
    "Timeout": 2880  # 48ì‹œê°„ (FLEXëŠ” ë” ê¸´ íƒ€ì„ì•„ì›ƒ í—ˆìš©)
}
```

**B. ë¹„ìš© ì ˆì•½ ë©”ì»¤ë‹ˆì¦˜:**

**AWSì˜ ìœ íœ´ ìš©ëŸ‰ í™œìš©:**
```
Standard í´ë˜ìŠ¤:
ìš”ì²­ ì¦‰ì‹œ â†’ ì „ìš© ë¦¬ì†ŒìŠ¤ í• ë‹¹ â†’ í‘œì¤€ ìš”ê¸ˆ ($0.44/DPU-hour)

FLEX í´ë˜ìŠ¤:  
ìš”ì²­ ëŒ€ê¸° â†’ ìœ íœ´ ìš©ëŸ‰ í™œìš© â†’ 50% í• ì¸ ìš”ê¸ˆ ($0.22/DPU-hour)
```

**ì‹¤í–‰ ì§€ì—° ë²”ìœ„:**
- **ì¼ë°˜ì  ì§€ì—°**: 0~30ë¶„ (ëŒ€ë¶€ë¶„ì˜ ê²½ìš°)
- **ìµœëŒ€ ì§€ì—°**: ìˆ˜ ì‹œê°„ (ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ì‹œ)
- **SLA**: íŠ¹ì • ì‹œê°„ ë³´ì¥ ì—†ìŒ

**C. ì–¸ì œ FLEXë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ê°€:**

**ì í•©í•œ ìƒí™©:**
- âœ… **ë°°ì¹˜ ETL ì‘ì—…**: íŠ¹ì • ì‹œê°„ ì™„ë£Œ ë¶ˆí•„ìš”
- âœ… **ì¼ì¼/ì£¼ê°„ ì •ê¸° ì‘ì—…**: ìœ ì—°í•œ ì‹¤í–‰ ì‹œê°„ í—ˆìš©
- âœ… **ë¹„ìš© ë¯¼ê°í•œ ì›Œí¬ë¡œë“œ**: 50% ë¹„ìš© ì ˆì•½ ì¤‘ìš”
- âœ… **ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½**: ì¦‰ì‹œ ì‹¤í–‰ ë¶ˆí•„ìš”

**ë¶€ì í•©í•œ ìƒí™©:**
- âŒ **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ì¦‰ì‹œ ì‹¤í–‰ í•„ìš”
- âŒ **SLA ìš”êµ¬ì‚¬í•­**: ì •í™•í•œ ì™„ë£Œ ì‹œê°„ í•„ìš”
- âŒ **ì˜ì¡´ì„± ìˆëŠ” ì‘ì—…**: ë‹¤ë¥¸ ì‘ì—…ì´ ê²°ê³¼ ëŒ€ê¸°

**D. FLEX í´ë˜ìŠ¤ ì„¤ì • ë° ëª¨ë‹ˆí„°ë§:**

**CloudFormation í…œí”Œë¦¿:**
```yaml
GlueJob:
  Type: AWS::Glue::Job
  Properties:
    Name: FlexETLJob
    ExecutionClass: FLEX
    GlueVersion: "3.0"
    MaxCapacity: 5.0
    Command:
      Name: glueetl
      ScriptLocation: s3://bucket/scripts/etl-script.py
    DefaultArguments:
      "--enable-metrics": ""
      "--enable-continuous-cloudwatch-log": "true"
```

**ë¹„ìš© ëª¨ë‹ˆí„°ë§:**
```python
import boto3

# Glue ì‘ì—… ì‹¤í–‰ ë¹„ìš© ì¶”ì 
glue = boto3.client('glue')

def get_job_cost_comparison():
    job_runs = glue.get_job_runs(JobName='etl-job')['JobRuns']
    
    for run in job_runs:
        execution_time = run['ExecutionTime']  # ë¶„ ë‹¨ìœ„
        dpu_capacity = run['MaxCapacity']
        
        if run.get('ExecutionClass') == 'FLEX':
            cost = (execution_time / 60) * dpu_capacity * 0.22  # FLEX ìš”ê¸ˆ
            print(f"FLEX ì‹¤í–‰ ë¹„ìš©: ${cost:.2f}")
        else:
            cost = (execution_time / 60) * dpu_capacity * 0.44  # Standard ìš”ê¸ˆ
            print(f"Standard ì‹¤í–‰ ë¹„ìš©: ${cost:.2f}")
```

**E. ì‹¤ì œ ë¹„ìš© ì ˆì•½ ì‚¬ë¡€:**

**ì¼ì¼ ETL ì‘ì—… (5 DPU, 2ì‹œê°„ ì‹¤í–‰):**
```
Standard í´ë˜ìŠ¤:
ë¹„ìš© = 2ì‹œê°„ Ã— 5 DPU Ã— $0.44 = $4.4/ì¼
ì›” ë¹„ìš© = $4.4 Ã— 30ì¼ = $132/ì›”

FLEX í´ë˜ìŠ¤:
ë¹„ìš© = 2ì‹œê°„ Ã— 5 DPU Ã— $0.22 = $2.2/ì¼  
ì›” ë¹„ìš© = $2.2 Ã— 30ì¼ = $66/ì›”

ì—°ê°„ ì ˆì•½ì•¡ = ($132 - $66) Ã— 12ê°œì›” = $792
```

**ëŒ€ê·œëª¨ ë°°ì¹˜ ì‘ì—… (20 DPU, 4ì‹œê°„ ì‹¤í–‰, ì£¼ 3íšŒ):**
```
Standard: 4ì‹œê°„ Ã— 20 DPU Ã— $0.44 Ã— 3íšŒ Ã— 52ì£¼ = $5,478/ë…„
FLEX: 4ì‹œê°„ Ã— 20 DPU Ã— $0.22 Ã— 3íšŒ Ã— 52ì£¼ = $2,739/ë…„
ì ˆì•½ì•¡ = $2,739/ë…„ (50% ì ˆì•½)
```

**F. ëª¨ë²” ì‚¬ë¡€:**

**FLEX ì‘ì—… ìµœì í™”:**
```python
# FLEX ì‘ì—…ì— ì í•©í•œ ì„¤ì •
job_args = {
    "--enable-metrics": "",                    # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    "--enable-continuous-cloudwatch-log": "",  # ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°
    "--enable-spark-ui": "",                   # Spark UI í™œì„±í™”
    "--job-max-capacity": "10",                # ì ì ˆí•œ DPU ì„¤ì •
    "--conf": "spark.sql.adaptive.enabled=true"  # ì ì‘í˜• ì¿¼ë¦¬ ì‹¤í–‰
}
```

**ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡:**
- **íˆìŠ¤í† ë¦¬ ë¶„ì„**: ê³¼ê±° ì‹¤í–‰ íŒ¨í„´ìœ¼ë¡œ ì§€ì—° ì‹œê°„ ì˜ˆì¸¡
- **ë²„í¼ ì‹œê°„**: ì˜ì¡´ì„± ìˆëŠ” í›„ì† ì‘ì—…ì— ì¶©ë¶„í•œ ë²„í¼ ì œê³µ
- **ì•Œë¦¼ ì„¤ì •**: ì˜ˆìƒë³´ë‹¤ ì˜¤ë˜ ì§€ì—°ë˜ëŠ” ê²½ìš° ì•ŒëŒ

ì‹œí—˜ í¬ì¸íŠ¸:
- **ë¹„ìš© ìµœì í™”** = FLEX ì‹¤í–‰ í´ë˜ìŠ¤ë¡œ 50% ì ˆì•½
- **ìœ ì—°í•œ ì‹¤í–‰ ì‹œê°„** = íŠ¹ì • ì‹œê°„ ì™„ë£Œ ë¶ˆí•„ìš”í•œ ì‘ì—…
- **ìœ íœ´ ìš©ëŸ‰ í™œìš©** = AWS ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì  ì‚¬ìš©

### 7. Amazon Athena Workgroup - Spark ì—”ì§„ ì§€ì›

**ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤**: Athenaì—ì„œ CTASë¥¼ ì‚¬ìš©í•œ SQL ETLì„ Apache Sparkë¥¼ ì‚¬ìš©í•œ ë¶„ì„ìœ¼ë¡œ ì „í™˜, Athenaì—ì„œ Spark ì ‘ê·¼ ë°©ë²•

**ì •ë‹µ: Athena workgroup (Spark ì—”ì§„ ì§€ì›)**

**A. Athena Workgroupì´ë€?**

**ì›Œí¬ê·¸ë£¹ í•µì‹¬ ê°œë…:**
- **ì¿¼ë¦¬ ì‹¤í–‰ í™˜ê²½**: ì‚¬ìš©ìë³„/íŒ€ë³„ ê²©ë¦¬ëœ ì‹¤í–‰ í™˜ê²½
- **ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**: ì¿¼ë¦¬ë‹¹ ë°ì´í„° ìŠ¤ìº” ì œí•œ, ë¹„ìš© ì œì–´
- **ì—”ì§„ ì„ íƒ**: SQL ì—”ì§„ ë˜ëŠ” Apache Spark ì—”ì§„
- **ì„¤ì • ê´€ë¦¬**: ê²°ê³¼ ìœ„ì¹˜, ì•”í˜¸í™”, íƒœê·¸ ë“± ì¤‘ì•™ ê´€ë¦¬

**B. Spark ì§€ì› Athena for Analytics:**

**Athena for Apache Spark ì„¤ì •:**
```python
import boto3

athena = boto3.client('athena')

# Spark ì—”ì§„ ì›Œí¬ê·¸ë£¹ ìƒì„±
response = athena.create_work_group(
    Name='spark-analytics-workgroup',
    Description='Spark engine for advanced analytics',
    Configuration={
        'ResultConfiguration': {
            'OutputLocation': 's3://athena-results-bucket/spark-outputs/'
        },
        'EnforceWorkGroupConfiguration': True,
        'PublishCloudWatchMetrics': True,
        'EngineVersion': {
            'SelectedEngineVersion': 'Spark engine version 3',
            'EffectiveEngineVersion': 'Spark engine version 3'
        }
    }
)
```

**C. Spark ë…¸íŠ¸ë¶ ì¸í„°í˜ì´ìŠ¤:**

**Jupyter ë…¸íŠ¸ë¶ í†µí•©:**
```python
# Athena Spark ì„¸ì…˜ ì‹œì‘
from pyathena import connect
from pyathena.spark.session import get_spark_session

# Spark ì„¸ì…˜ ìƒì„±
spark = get_spark_session(
    work_group='spark-analytics-workgroup',
    s3_staging_dir='s3://athena-spark-staging/'
)

# DataFrame API ì‚¬ìš©
df = spark.sql("""
    SELECT customer_id, product_category, 
           SUM(amount) as total_spent
    FROM sales_data 
    WHERE order_date >= '2024-01-01'
    GROUP BY customer_id, product_category
""")

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans

assembler = VectorAssembler(
    inputCols=['total_spent', 'order_frequency'],
    outputCol='features'
)

feature_df = assembler.transform(df)
kmeans = KMeans(k=5, seed=1)
model = kmeans.fit(feature_df)
```

**D. SQL vs Spark ë¹„êµ:**

**ê¸°ì¡´ SQL CTAS ë°©ì‹:**
```sql
-- ì œí•œì ì¸ ë¶„ì„ ê¸°ëŠ¥
CREATE TABLE customer_segments AS
SELECT customer_id, 
       AVG(order_amount) as avg_order,
       COUNT(*) as order_count
FROM orders 
GROUP BY customer_id;
```

**Spark ë°©ì‹:**
```python
# ê³ ê¸‰ ë¶„ì„ ë° ë¨¸ì‹ ëŸ¬ë‹
from pyspark.ml.stat import Correlation
from pyspark.ml.feature import StandardScaler

# í†µê³„ ë¶„ì„
correlation_matrix = Correlation.corr(df, 'features').head()[0]

# ì‹¤ì‹œê°„ ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")
scaler_model = scaler.fit(df)
scaled_df = scaler_model.transform(df)

# ë³µì¡í•œ ìœˆë„ìš° í•¨ìˆ˜
from pyspark.sql.window import Window
from pyspark.sql.functions import lag, lead

window_spec = Window.partitionBy('customer_id').orderBy('order_date')
df_with_trends = df.withColumn('prev_amount', lag('amount').over(window_spec))
```

**E. ì›Œí¬ê·¸ë£¹ í™œìš© ì‹œë‚˜ë¦¬ì˜¤:**

**íŒ€ë³„ ë¶„ë¦¬:**
```python
# ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ íŒ€ ì›Œí¬ê·¸ë£¹
ds_workgroup = {
    'Name': 'data-science-team',
    'Configuration': {
        'BytesScannedCutoffPerQuery': 1000000000000,  # 1TB ì œí•œ
        'EnforceWorkGroupConfiguration': True,
        'EngineVersion': 'Spark engine version 3'
    }
}

# BI ë¶„ì„ê°€ ì›Œí¬ê·¸ë£¹  
bi_workgroup = {
    'Name': 'business-analysts',
    'Configuration': {
        'BytesScannedCutoffPerQuery': 100000000000,   # 100GB ì œí•œ
        'EnforceWorkGroupConfiguration': True,
        'EngineVersion': 'Athena engine version 2'    # SQL ì—”ì§„
    }
}
```

**ë¹„ìš© ì œì–´:**
```python
# ì›Œí¬ê·¸ë£¹ë³„ ë¹„ìš© ëª¨ë‹ˆí„°ë§
def monitor_workgroup_costs(workgroup_name):
    cloudwatch = boto3.client('cloudwatch')
    
    # ë°ì´í„° ìŠ¤ìº” ë¹„ìš© ì¶”ì 
    response = cloudwatch.get_metric_statistics(
        Namespace='AWS/Athena',
        MetricName='DataScannedInBytes',
        Dimensions=[
            {'Name': 'WorkGroup', 'Value': workgroup_name}
        ],
        StartTime=datetime.utcnow() - timedelta(days=30),
        EndTime=datetime.utcnow(),
        Period=86400,  # ì¼ë³„
        Statistics=['Sum']
    )
    
    # ë¹„ìš© ê³„ì‚° ($5 per TB scanned)
    total_bytes = sum(point['Sum'] for point in response['Datapoints'])
    cost = (total_bytes / (1024**4)) * 5  # TB ë‹¹ $5
    
    return cost
```

**F. Spark ì—”ì§„ì˜ ì¥ì :**

**ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥:**
- âœ… **ë¨¸ì‹ ëŸ¬ë‹**: MLlib ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©
- âœ… **ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬**: ì¤€ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬  
- âœ… **ê·¸ë˜í”„ ë¶„ì„**: GraphX API ì§€ì›
- âœ… **ë³µì¡í•œ ETL**: ë‹¤ë‹¨ê³„ ë³€í™˜ íŒŒì´í”„ë¼ì¸

**í™•ì¥ì„±:**
```python
# ìë™ ìŠ¤ì¼€ì¼ë§
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")  
spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "128MB")
```

**í†µí•©ì„±:**
- **S3 ë„¤ì´í‹°ë¸Œ**: Parquet, Delta Lake í˜•ì‹ ì§€ì›
- **Glue Catalog**: ë©”íƒ€ë°ì´í„° ìë™ í†µí•©
- **IAM**: ì„¸ë°€í•œ ì ‘ê·¼ ê¶Œí•œ ì œì–´

ì‹œí—˜ í¬ì¸íŠ¸:
- **Athena Spark ì§€ì›** = ì›Œí¬ê·¸ë£¹ì˜ ì—”ì§„ ì„¤ì •
- **SQLì—ì„œ Spark ì „í™˜** = ê³ ê¸‰ ë¶„ì„ ë° ë¨¸ì‹ ëŸ¬ë‹ í™œìš©
- **íŒ€ë³„ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬** = ì›Œí¬ê·¸ë£¹ ê¸°ë°˜ ë¹„ìš©/ê¶Œí•œ ì œì–´